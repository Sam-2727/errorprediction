- The authors try to infer the uncertainties of a set of measurements without
 quoted error bars from a set of measurements with quoted error bars.  This
 can only reasonably work if there are good arguments why the two sets of
 measurements are otherwise completely identical.  This means that the
 underlying systematics must be identical, and the measurement result itself
 did not influence the decision whether an error bar was quoted in the
 literature or not.  This is central for what the authors aim to do.  However,
 they do not provide any evidence that this is the case for the data sets they
 study.

Answer: The underlying systematics are identical, because we attempt this prediction only for a subset of measurements in NED-D for which the same method for determining distances is used (TFR). Additionally, our error prediction for missing TF errors in HyperLEDA using the NED-D-based model coincide very well with estimated TF errors in HyperLEDA, even though the the NED-D model is completely independent of HyperLEDA. We believe this shows that the model is indeed learning features that are related to systematic errors in the TF method for distance determination. This discussion was relegated to the appendix originally, but because of its importance we moved this discussion to the main body of the manuscript for this corrected version.


- It is evident from Fig. 1 and Fig. 3 that the distance uncertainties from
 model H (at e.g. 200 Mpc) depends on the number of underlying measurements.
 I would like to see how these plots look for N>15 or N>20, similar to what is
 used in the Bayesian linear and quadratic models.  The fact that the error
 bars grow as function of N ... 

Answer: The scale of the error (relative error) does not depend on the limiting number of measurements. We now include a D vs sigma plot (Figure XXX-new) which shows that the slope remains similar even for galaxies with a large number of distance measurements. 

- ... suggests that individual errors of the distance
 measures are somewhat systematically underestimated.   This point is
 important and must be discussed.  

Answer: Frequentist methods both over- and underestimate errors, as Fig. XXX shows. If they were appropriate measures of the scatter, they would coincide better with H/M error trends. This is now made clear in the text.

- If anything, this trend should be the
 opposite (more measurements should increase our knowledge, not decrease it),
 and indicates that results with a small number of measruements N cannot be
 trusted.

Answer: In multi-measurement catalogs such as NED-D, we observe that the scatter of reported distances and reported individual measurement errors do not match in most cases. This situation happens because there are hidden systematic sources. These systematics can not be removed, but they can be margizalized over in order to estimate the true variance of a distance estimation based on multiple measurements. In this case, more measurements can increase our knowledge of systematic uncertainties in distance measurements, which is what we do when we attempt to predict unreported TF errors.

- It appears that the Bayesian quadratic model only works, in terms of the
 Bayesian p-value, if the observations are constrainted to galaxies with N>25
 measurements.  Otherwise the model overfits (p~1) or underfits (p~0) the
 data.  

Answer: The quadratic model works under very limited conditions, and the posterior shows that the random error component is degenerate (and ultimately unnecesary, as the mode/median is close to zero), which is why we decide to use a simpler linear model. The degenerate parameter included in the quadratic model increases the randomness of the error around the expected values of the probability of each galaxy, which is why the linear model works better (i.e. for more galaxies) at not overpredicting the error.

- The same is true for the Bayesian linear model, with other threshold
 values of N.  It is not clear how such a model could be generally useful,
 except for very specific cases.  

Answer: This is true, our model is only useful for predicting missing TF errors within the same distance range as the data points used to inform the model. Incidentally, most galaxies with missing TF errors are within or very near the distance range of the data points used to inform the model, as Fig. XXX (the one in the appendix). We do not attempt to predict missing distance errors for methods other than TFR, although we think it could be done in the future.

- Why does the model break down?  What happens
 to observations that are not fulfillying this requirement?

Answer: The model works at a high threshold N_TF because the Central Limit Theorem implies that "when independent random variables are added, their properly normalized sum tends toward a normal distribution [...] even if the original variables themselves are notnormally distributed". Thus, it is easier to estimate the systematic (true) variance from those measurements than from galaxies with a low number of measurements. Still, one should keep in mind that our predictions are almost worse case scenarios for unknown errors (the worst case scenario being having no errors at all).

- It is not clear why it is useful to estimate the variance of the predicted
 measurement uncertainty.  Typically one would be happy with the expected/mean
 uncertainty alone.  What is the variance of the uncertainty good for in
 practice?

Answer: This is shown only to justify the choice of the variance model (variance \propto D) in the likelihood.
 
- It is not clear to me what kind of errors the authors want to predict.  As
 discussed above, the size of the H model error seems to depend on the number
 of TF measurements N.  

Answer: The scale of the error (relative error) for TF distance measurements does not depend significantly on N_TF. We now include a D vs sigma plot (Figure XXX-newTF) which shows that the slope remains similar even for galaxies with a large number of distance measurements. 

 This means that the specific models that the authors
 derive (Bayesian linear and quadratic models) only work for observations of
 galaxies with a similar number of N.  It is not clear whether the 818
 galaxies for which they use their results actually fulfill this criterion.
 Furthermore, it is clear that the results cannot be generalized to galaxies
 with a smaller number of distance measurements, which means that the results
 are of limited use.

Answer: It is true that the success of the predictive model depends on N (answer #XXX). However, our prediction is not based on any significant assumption about the galaxies, other than the fact that the distance range changes with N. The galaxies for which the linear model works do not have particular physical properties that make them special, other than having a bias towards closer distances, which we include in our reported prediction (with a * which indicates that the galaxy is outside of the predictive distance range). The 818 galaxies for which we predict errors previously had no reported errors, and our systematics-based prediction is better than having no errors at all. 


Minor concerns
--------------

- For the 818 galaxies of the NED-D that have TF distances without quoted
 distance errors, it is important know how many measurements (without distance
 errors) were actually done per galaxy.

Answer: About 50% have more than one TF measurement. The most TF measurements are 6. However, as mentioned in the answer above, the number of measurements only indicates a working distance range, which we consider in our model. Furthermore, we indicate when predictions are outside of this working range in Figs. XXX-predictions.

- The authors use as starting point for combining potentially discrepant
 distance measures a Gaussian mixture model, giving equal weight to all of the
 measurements (beginning of Section 2).  Model H (median and 1-sigma
 percentiles of that Gaussian mixture model) is the claimed to be the optimal
 or most faithful representation of the associated errors.  In my eyes, these
 are already very specific assumptions, and the authors should comment on the
 validty of these assumptions in context of the observations.

Answer: If the frequentist P and/or Q errors were representative of the uncertainty in distance measurement (as it is commonly asserted, cf. Cosmicflows), they would coincide with the results obtained using the H and/or M methods for error estimation. The fact that in general they differ shows that P/Q errors are insufficiently accurate measurements of the uncertainty in distance measurement.
 
- Given that error bars for the given problem are inherently non-symmetric, I
 do not understand why instead symmetric error bars are chosen for the four
 models.

Answer: We now also report H errors in our data tables in terms of distinct lower and upper limits. M, P, and Q errors are by definition symmetric.

To do: Calculate upper and lower limits.

- Fig. 5: It is not clear how the bootstrap realizations were calculated.  Is
 this somehow using the same number of measurements per galaxy that were in
 the original galaxy catalogue?

Answer: Yes, as indicated in Eq. %%%%%

To do: Give number to equation with sum of lognormals. Also, say in the text that this is the distribution we sample. Also, make clear that symmetric is only an approximation at lage N due to CLT.

- In Fig. 6, I'm wondering whether the p-value is correctly defined.  A larger
 p-value (red points) seem to correspond to more points with a larger
 simulated discrepancy.  This is directly opposite to the definition of the
 Bayesian p-value.

Answer: The Bayesian p-value for the red points is closer to 0.5 than the Bayesian p-values for larger N, which is the goal of this test.

- In Fig. 6, larger N correspond to larger p-values, in Fig. 9, larger N
 correspond to smaller p-values.  Does this fit together? 

Answer: As mentioned above, the quadratic model tends to overpredict randomness, whereas the opposite is the case for the linear model.

List of to-do's:

Calculations/code:
DONE 1. Show distribution of galaxies with missing errors vs galaxies with errors (show that they are not different). Histogram.
DONE 2. Show how error changes with N. Do for TF and in general.
DONE 3. Get percentiles for non-reported TF galaxies.
DONE 9. Change N to N_TF
DONE 10. Change colors+markers in prediction
DONE 11. Try 2018 catalog - linear model
4. Calculate hmin and hmax
8. Check symmetry argument (compare H to hmin and hmax).
15. Try 2018 catalog - quad model
16. Redo stuff for 2018 catalog?
17. Compare 2018 and 2017 TF catalogs -> there's something fishy about lack of TF measurements.
18. Update original repo (plots in /mnras/)

Text:
5. Move discussion of hyperleda to text
6. Change text re: H errors not being symmetric. 
7. Argue in the text that all errors should tend to H and CLT argument for symmetry at large N.
12. Rewrite text to separate two parts of study
13. Change N to N_TF in text
14. Re-reference plots in answers.
